{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "reload(dataloader)\n",
    "from dataloader import VideoClassificationDataset\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'feats_dir': \"/home/wgar/NeXtVLAD.pytorch/data/UCF101_debug/train_PCA-1024\",\n",
    "    'max_frames': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load feats from /home/wgar/NeXtVLAD.pytorch/data/UCF101_debug/train_PCA-1024\n",
      "Pre-cache 309 features in memory.\n",
      "Finished initializing dataloader.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VideoClassificationDataset(opt, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                         batch_size=8,\n",
    "                         num_workers=4,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeXtVLAD(nn.Module):\n",
    "    \"\"\"NeXtVLAD layer implementation\"\"\"\n",
    "\n",
    "    def __init__(self, dim=1024, num_clusters=64, lamb=2, groups=8, max_frames=300):\n",
    "        super(NeXtVLAD, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.dim = dim\n",
    "        self.alpha = 0\n",
    "        self.K = num_clusters\n",
    "        self.G = groups\n",
    "        self.group_size = int((lamb*dim) // self.G)\n",
    "        # expansion FC\n",
    "        self.fc0 = nn.Linear(dim, lamb*dim)\n",
    "        # soft assignment FC (the cluster weights)\n",
    "        self.fc_gk = nn.Linear(lamb*dim, self.G * self.K)\n",
    "        # attention over groups FC\n",
    "        self.fc_g = nn.Linear(lamb*dim, self.G)\n",
    "        self.cluster_weights2 = nn.Parameter(torch.rand(1, self.group_size, self.K))\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(max_frames)\n",
    "        self.bn1 = nn.BatchNorm1d(1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "#         print(f\"x: {x.shape}\")\n",
    "    \n",
    "        _, M, N = x.shape\n",
    "        # expansion FC: B x M x N -> B x M x λN\n",
    "        x_dot = self.fc0(x) \n",
    "        \n",
    "        # reshape into groups: B x M x λN -> B x M x G x (λN/G)\n",
    "        x_tilde = x_dot.reshape(-1, M, self.G, self.group_size)\n",
    "        \n",
    "        # residuals across groups and clusters: B x M x λN -> B x M x (G*K) \n",
    "        WgkX = self.fc_gk(x_dot)\n",
    "        WgkX = self.bn0(WgkX)\n",
    "        \n",
    "        # residuals reshape across clusters: B x M x (G*K) -> B x (M*G) x K\n",
    "        WgkX = WgkX.reshape(-1, M*self.G, self.K)\n",
    "        \n",
    "        # softmax over assignment: B x (M*G) x K -> B x (M*G) x K\n",
    "        alpha_gk = F.softmax(WgkX, dim=-1)\n",
    "        \n",
    "        # attention across groups: B x M x λN -> B x M x G\n",
    "        alpha_g = torch.sigmoid(self.fc_g(x_dot))\n",
    "        if mask is not None:\n",
    "            alpha_g = torch.mul(alpha_g, mask.unsqueeze(2))\n",
    "        \n",
    "        # reshape across time: B x M x G -> B x (M*G) x 1\n",
    "        alpha_g = alpha_g.reshape(-1, M*self.G, 1)\n",
    "        \n",
    "        # apply attention: B x (M*G) x K (X) B x (M*G) x 1 -> B x (M*G) x K\n",
    "        activation = torch.mul(alpha_gk, alpha_g)\n",
    "        \n",
    "        # sum over time and group: B x (M*G) x K -> B x 1 x K\n",
    "        a_sum = torch.sum(activation, -2, keepdim=True)\n",
    "        \n",
    "        # calculate group centers: B x 1 x K (X) 1 x (λN/G) x K -> B x (λN/G) x K\n",
    "        a = torch.mul(a_sum, self.cluster_weights2)\n",
    "        \n",
    "        # permute: B x (M*G) x K -> B x K x (M*G)\n",
    "        activation = activation.permute(0, 2, 1)\n",
    "        \n",
    "        # reshape: B x M x G x (λN/G) -> B x (M*G) x (λN/G)\n",
    "        reshaped_x_tilde = x_tilde.reshape(-1, M * self.G, self.group_size)\n",
    "        \n",
    "        # cluster activation: B x K x (M*G) (X) B x (M*G) x (λN/G) -> B x K x (λN/G)\n",
    "        vlad = torch.matmul(activation, reshaped_x_tilde)\n",
    "        # print(f\"vlad: {vlad.shape}\")\n",
    "        \n",
    "        # permute: B x K x (λN/G) (X) B x (λN/G) x K\n",
    "        vlad = vlad.permute(0, 2, 1)\n",
    "        # distance to centers: B x (λN/G) x K (-) B x (λN/G) x K\n",
    "        vlad = torch.sub(vlad, a)\n",
    "        # normalize: B x (λN/G) x K\n",
    "        vlad = F.normalize(vlad, 1)\n",
    "        # reshape: B x (λN/G) x K -> B x 1 x (K * (λN/G))\n",
    "        vlad = vlad.reshape(-1, 1, self.K*self.group_size)\n",
    "        vlad = self.bn1(vlad)\n",
    "        # reshape:  B x 1 x (K * (λN/G)) -> B x (K * (λN/G)) \n",
    "        vlad = vlad.reshape(-1, self.K*self.group_size)\n",
    "        \n",
    "        return vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeXtVLADModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_clusters=64, dim=1024, lamb=2, hidden_size=1024, \n",
    "                 groups=8, max_frames=300, drop_rate=0.5, gating_reduction=8):\n",
    "        super(NeXtVLADModel, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.group_size = int((lamb*dim) // groups)\n",
    "        self.fc0 = nn.Linear(num_clusters*self.group_size, hidden_size)\n",
    "        self.bn0 = nn.BatchNorm1d(1)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // gating_reduction)\n",
    "        self.bn1 = nn.BatchNorm1d(1)\n",
    "        self.fc2 = nn.Linear(hidden_size // gating_reduction, hidden_size)\n",
    "        self.logistic = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.video_nextvlad = NeXtVLAD(1024, max_frames=max_frames, lamb=lamb, \n",
    "                                       num_clusters=num_clusters, groups=groups)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # B x M x N -> B x (K * (λN/G)) \n",
    "        vlad = self.video_nextvlad(x, mask=mask)\n",
    "        \n",
    "        # B x (K * (λN/G)) \n",
    "        if self.drop_rate > 0.:\n",
    "            vlad = F.dropout(vlad, p=self.drop_rate)\n",
    "        \n",
    "        # B x (K * (λN/G))  -> B x H0\n",
    "        activation = self.fc0(vlad)\n",
    "        activation = self.bn0(activation.unsqueeze(1)).squeeze()\n",
    "        activation = F.relu(activation)\n",
    "        # B x H0 -> B x Gr\n",
    "        gates = self.fc1(activation)\n",
    "        gates = self.bn1(gates.unsqueeze(1)).squeeze()\n",
    "        # B x Gr -> B x H0\n",
    "        gates = self.fc2(gates)\n",
    "        gates = torch.sigmoid(gates)\n",
    "        # B x H0\n",
    "        activation = torch.mul(activation, gates)\n",
    "        out = self.logistic(activation)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeXtVLADModel(train_dataset.num_classes, max_frames=opt['max_frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t0,\tloss:0.71150803565979\n",
      "epoch:\t0,\tloss:0.7146843075752258\n",
      "epoch:\t0,\tloss:0.5954272150993347\n",
      "epoch:\t0,\tloss:0.6422020196914673\n",
      "epoch:\t0,\tloss:0.5928784012794495\n",
      "epoch:\t0,\tloss:0.5122247338294983\n",
      "epoch:\t0,\tloss:0.6339268088340759\n",
      "epoch:\t0,\tloss:0.6461817026138306\n",
      "epoch:\t0,\tloss:0.5485246777534485\n",
      "epoch:\t0,\tloss:0.4685041010379791\n",
      "epoch:\t0,\tloss:0.47569671273231506\n",
      "epoch:\t0,\tloss:0.44309744238853455\n",
      "epoch:\t0,\tloss:0.4427070617675781\n",
      "epoch:\t0,\tloss:0.4889450967311859\n",
      "epoch:\t0,\tloss:0.4677741527557373\n",
      "epoch:\t0,\tloss:0.5337631106376648\n",
      "epoch:\t0,\tloss:0.26898637413978577\n",
      "epoch:\t0,\tloss:0.4407520294189453\n",
      "epoch:\t0,\tloss:0.3478405773639679\n",
      "epoch:\t0,\tloss:0.3842126131057739\n",
      "epoch:\t0,\tloss:0.4324081242084503\n",
      "epoch:\t0,\tloss:0.3600882589817047\n",
      "epoch:\t0,\tloss:0.23256240785121918\n",
      "epoch:\t0,\tloss:0.3121560513973236\n",
      "epoch:\t0,\tloss:0.30288824439048767\n",
      "epoch:\t0,\tloss:0.17022843658924103\n",
      "epoch:\t0,\tloss:0.2929340898990631\n",
      "epoch:\t0,\tloss:0.12832005321979523\n",
      "epoch:\t0,\tloss:0.27824535965919495\n",
      "epoch:\t0,\tloss:0.1415291577577591\n",
      "epoch:\t0,\tloss:0.10328144580125809\n",
      "epoch:\t0,\tloss:0.18037013709545135\n",
      "epoch:\t0,\tloss:0.09852627664804459\n",
      "epoch:\t0,\tloss:0.24034304916858673\n",
      "epoch:\t0,\tloss:0.19834274053573608\n",
      "epoch:\t0,\tloss:0.18170402944087982\n",
      "epoch:\t0,\tloss:0.09749788045883179\n",
      "epoch:\t0,\tloss:0.11607547849416733\n",
      "epoch:\t0,\tloss:0.0376109853386879\n",
      "epoch:\t1,\tloss:0.09225022047758102\n",
      "epoch:\t1,\tloss:0.02818330191075802\n",
      "epoch:\t1,\tloss:0.01991969160735607\n",
      "epoch:\t1,\tloss:0.027169905602931976\n",
      "epoch:\t1,\tloss:0.03678622841835022\n",
      "epoch:\t1,\tloss:0.025325099006295204\n",
      "epoch:\t1,\tloss:0.02467428892850876\n",
      "epoch:\t1,\tloss:0.02342064119875431\n",
      "epoch:\t1,\tloss:0.006782206241041422\n",
      "epoch:\t1,\tloss:0.01385602355003357\n",
      "epoch:\t1,\tloss:0.025996623560786247\n",
      "epoch:\t1,\tloss:0.05951960012316704\n",
      "epoch:\t1,\tloss:0.02473391406238079\n",
      "epoch:\t1,\tloss:0.009166402742266655\n",
      "epoch:\t1,\tloss:0.014516142196953297\n",
      "epoch:\t1,\tloss:0.008172743953764439\n",
      "epoch:\t1,\tloss:0.00869485829025507\n",
      "epoch:\t1,\tloss:0.007238415535539389\n",
      "epoch:\t1,\tloss:0.015679262578487396\n",
      "epoch:\t1,\tloss:0.005326947197318077\n",
      "epoch:\t1,\tloss:0.004706756677478552\n",
      "epoch:\t1,\tloss:0.0034857578575611115\n",
      "epoch:\t1,\tloss:0.004158268216997385\n",
      "epoch:\t1,\tloss:0.0059410869143903255\n",
      "epoch:\t1,\tloss:0.006137473974376917\n",
      "epoch:\t1,\tloss:0.027485201135277748\n",
      "epoch:\t1,\tloss:0.0030662603676319122\n",
      "epoch:\t1,\tloss:0.0027211287524551153\n",
      "epoch:\t1,\tloss:0.005441123154014349\n",
      "epoch:\t1,\tloss:0.002412184840068221\n",
      "epoch:\t1,\tloss:0.003415692364796996\n",
      "epoch:\t1,\tloss:0.002235227031633258\n",
      "epoch:\t1,\tloss:0.0033596300054341555\n",
      "epoch:\t1,\tloss:0.005315082613378763\n",
      "epoch:\t1,\tloss:0.002937655197456479\n",
      "epoch:\t1,\tloss:0.002290458185598254\n",
      "epoch:\t1,\tloss:0.007688215002417564\n",
      "epoch:\t1,\tloss:0.0027433286886662245\n",
      "epoch:\t1,\tloss:0.002007373608648777\n",
      "epoch:\t2,\tloss:0.0023477952927351\n",
      "epoch:\t2,\tloss:0.001861131633631885\n",
      "epoch:\t2,\tloss:0.0010836547007784247\n",
      "epoch:\t2,\tloss:0.0018228593980893493\n",
      "epoch:\t2,\tloss:0.002600134816020727\n",
      "epoch:\t2,\tloss:0.0016692288918420672\n",
      "epoch:\t2,\tloss:0.0022326342295855284\n",
      "epoch:\t2,\tloss:0.002286992035806179\n",
      "epoch:\t2,\tloss:0.0010390046518296003\n",
      "epoch:\t2,\tloss:0.0015219493070617318\n",
      "epoch:\t2,\tloss:0.0020285442005842924\n",
      "epoch:\t2,\tloss:0.001638404093682766\n",
      "epoch:\t2,\tloss:0.0016711241332814097\n",
      "epoch:\t2,\tloss:0.001292763277888298\n",
      "epoch:\t2,\tloss:0.0024440630804747343\n",
      "epoch:\t2,\tloss:0.0009746658033691347\n",
      "epoch:\t2,\tloss:0.0018626617966219783\n",
      "epoch:\t2,\tloss:0.0014337325701490045\n",
      "epoch:\t2,\tloss:0.0010064152302220464\n",
      "epoch:\t2,\tloss:0.0013640819815918803\n",
      "epoch:\t2,\tloss:0.0010625479044392705\n",
      "epoch:\t2,\tloss:0.0014420481165871024\n",
      "epoch:\t2,\tloss:0.0008912244811654091\n",
      "epoch:\t2,\tloss:0.001545345294289291\n",
      "epoch:\t2,\tloss:0.0010661480482667685\n",
      "epoch:\t2,\tloss:0.0008565050084143877\n",
      "epoch:\t2,\tloss:0.0006735824863426387\n",
      "epoch:\t2,\tloss:0.0008698648889549077\n",
      "epoch:\t2,\tloss:0.0017080713296309114\n",
      "epoch:\t2,\tloss:0.0010185097344219685\n",
      "epoch:\t2,\tloss:0.0010622901609167457\n",
      "epoch:\t2,\tloss:0.0012533634435385466\n",
      "epoch:\t2,\tloss:0.0009148705867119133\n",
      "epoch:\t2,\tloss:0.0006462182500399649\n",
      "epoch:\t2,\tloss:0.0005030953325331211\n",
      "epoch:\t2,\tloss:0.0015385170700028539\n",
      "epoch:\t2,\tloss:0.0006630505085922778\n",
      "epoch:\t2,\tloss:0.0007098554633557796\n",
      "epoch:\t2,\tloss:0.0009038225398398936\n",
      "epoch:\t3,\tloss:0.00042479473631829023\n",
      "epoch:\t3,\tloss:0.0005855750059708953\n",
      "epoch:\t3,\tloss:0.0007057485054247081\n",
      "epoch:\t3,\tloss:0.0008201799355447292\n",
      "epoch:\t3,\tloss:0.0005157763953320682\n",
      "epoch:\t3,\tloss:0.0008646969799883664\n",
      "epoch:\t3,\tloss:0.0009159119799733162\n",
      "epoch:\t3,\tloss:0.000726655765902251\n",
      "epoch:\t3,\tloss:0.0007182210683822632\n",
      "epoch:\t3,\tloss:0.0008668283117003739\n",
      "epoch:\t3,\tloss:0.0007231898489408195\n",
      "epoch:\t3,\tloss:0.0008738313335925341\n",
      "epoch:\t3,\tloss:0.0006608644616790116\n",
      "epoch:\t3,\tloss:0.0006918812287040055\n",
      "epoch:\t3,\tloss:0.00042746460530906916\n",
      "epoch:\t3,\tloss:0.0004623888526111841\n",
      "epoch:\t3,\tloss:0.00040710484609007835\n",
      "epoch:\t3,\tloss:0.0006149121909402311\n",
      "epoch:\t3,\tloss:0.0006414263043552637\n",
      "epoch:\t3,\tloss:0.0005345437093637884\n",
      "epoch:\t3,\tloss:0.0007223087013699114\n",
      "epoch:\t3,\tloss:0.0008337963372468948\n",
      "epoch:\t3,\tloss:0.0016031116247177124\n",
      "epoch:\t3,\tloss:0.0008548393961973488\n",
      "epoch:\t3,\tloss:0.0007479392806999385\n",
      "epoch:\t3,\tloss:0.0006933917757123709\n",
      "epoch:\t3,\tloss:0.0005947808967903256\n",
      "epoch:\t3,\tloss:0.00040444658952765167\n",
      "epoch:\t3,\tloss:0.0005790918366983533\n",
      "epoch:\t3,\tloss:0.0009108057129196823\n",
      "epoch:\t3,\tloss:0.0008470119792036712\n",
      "epoch:\t3,\tloss:0.0009477338171564043\n",
      "epoch:\t3,\tloss:0.00045438858796842396\n",
      "epoch:\t3,\tloss:0.0008903048583306372\n",
      "epoch:\t3,\tloss:0.0007609418244101107\n",
      "epoch:\t3,\tloss:0.001175822108052671\n",
      "epoch:\t3,\tloss:0.0005316018941812217\n",
      "epoch:\t3,\tloss:0.0006653064046986401\n",
      "epoch:\t3,\tloss:0.00032494479091838\n",
      "epoch:\t4,\tloss:0.0005201410385780036\n",
      "epoch:\t4,\tloss:0.0007186997099779546\n",
      "epoch:\t4,\tloss:0.00048609552322886884\n",
      "epoch:\t4,\tloss:0.0008609591168351471\n",
      "epoch:\t4,\tloss:0.0006337621598504484\n",
      "epoch:\t4,\tloss:0.00048226353828795254\n",
      "epoch:\t4,\tloss:0.0005028933519497514\n",
      "epoch:\t4,\tloss:0.00029791248380206525\n",
      "epoch:\t4,\tloss:0.0005183366592973471\n",
      "epoch:\t4,\tloss:0.00031539611518383026\n",
      "epoch:\t4,\tloss:0.00048409486771561205\n",
      "epoch:\t4,\tloss:0.00035559770185500383\n",
      "epoch:\t4,\tloss:0.0006230109720490873\n",
      "epoch:\t4,\tloss:0.0006612534634768963\n",
      "epoch:\t4,\tloss:0.00029597230604849756\n",
      "epoch:\t4,\tloss:0.0006362967542372644\n",
      "epoch:\t4,\tloss:0.00038377134478650987\n",
      "epoch:\t4,\tloss:0.0007281986181624234\n",
      "epoch:\t4,\tloss:0.0004282900772523135\n",
      "epoch:\t4,\tloss:0.00039028548053465784\n",
      "epoch:\t4,\tloss:0.0003747685404960066\n",
      "epoch:\t4,\tloss:0.0005309387925080955\n",
      "epoch:\t4,\tloss:0.000556213257368654\n",
      "epoch:\t4,\tloss:0.0005487402086146176\n",
      "epoch:\t4,\tloss:0.0003494209668133408\n",
      "epoch:\t4,\tloss:0.0006299832020886242\n",
      "epoch:\t4,\tloss:0.0004588236042764038\n",
      "epoch:\t4,\tloss:0.0005549622583203018\n",
      "epoch:\t4,\tloss:0.00018302483658771962\n",
      "epoch:\t4,\tloss:0.00024095167464111\n",
      "epoch:\t4,\tloss:0.0005101535934954882\n",
      "epoch:\t4,\tloss:0.00034454729757271707\n",
      "epoch:\t4,\tloss:0.00025429722154513\n",
      "epoch:\t4,\tloss:0.0002479896356817335\n",
      "epoch:\t4,\tloss:0.0007369245286099613\n",
      "epoch:\t4,\tloss:0.00034910847898572683\n",
      "epoch:\t4,\tloss:0.0005303460056893528\n",
      "epoch:\t4,\tloss:0.0005001642857678235\n",
      "epoch:\t4,\tloss:0.0002676190924830735\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "exp_lr_schedulr = optim.lr_scheduler.StepLR(optimizer, step_size=25)\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for data in train_loader:\n",
    "        fc_feats = data['fc_feats'].to(device)\n",
    "        labels = data['ground_truth'].to(device)\n",
    "        masks = data['mask'].to(device)\n",
    "\n",
    "        out = model(fc_feats, mask=masks)\n",
    "    #     print(f\"out: {out.shape}\")\n",
    "    #     print(f\"labels: {labels.shape}\")\n",
    "        loss = F.binary_cross_entropy(out, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"epoch:\\t{epoch},\\tloss:{loss.cpu().data.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "data = train_dataset.__getitem__(5)\n",
    "fc_feats = data['fc_feats']\n",
    "print(fc_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import metrics\n",
    "reload(metrics)\n",
    "from metrics import calculate_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load feats from /home/wgar/NeXtVLAD.pytorch/data/UCF101_debug/test_PCA-1024\n",
      "Pre-cache 75 features in memory.\n",
      "Finished initializing dataloader.\n"
     ]
    }
   ],
   "source": [
    "opt = {\n",
    "    'feats_dir': \"/home/wgar/NeXtVLAD.pytorch/data/UCF101_debug/test_PCA-1024\",\n",
    "    'max_frames': 50\n",
    "}\n",
    "test_dataset = VideoClassificationDataset(opt, 'test')\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=8,\n",
    "                         num_workers=4,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAP(20): 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "actuals = []\n",
    "\n",
    "for data in test_loader:\n",
    "    fc_feats = data['fc_feats'].to(device)\n",
    "    labels = data['ground_truth']\n",
    "    masks = data['mask'].to(device)\n",
    "\n",
    "    out = model(fc_feats, mask=masks)\n",
    "    out = out.cpu().data.numpy()\n",
    "    labels = labels.cpu().data.numpy()\n",
    "#     print(out.shape)\n",
    "#     print(labels.shape)\n",
    "    preds.extend(out)\n",
    "    actuals.extend(labels)\n",
    "    \n",
    "print(f\"GAP(20): {calculate_gap(np.asarray(preds), np.asarray(actuals), top_k=20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
